我们现在来说明技术伦理学的一些主题。我们将重点放在一些一般性的主题上，这些主题为技术伦理学中的一般性问题以及处理这些问题的方式提供了说明。

#### 技术中立vs道德代理（moral agency）

> 注：这里的“代理”可以理解为“行动主体”。我没有找到恰切的中文词汇与其对应。

技术伦理学领域有一个重要的一般性主题：技术是否蕴含价值取向（value-laden）。一些作者认为，技术是价值中立的，即技术只是达到目的的一种中性手段，可能有好的用途也可能有坏的用途(例如，Pitt 2000)。这种观点可能有一定的合理性，因为技术被认为只是单纯的物理性结构。然而，大多数技术哲学家都认为，技术发展是一个目标导向的过程，技术人工制品从定义上讲具有一定的功能，因此它们可以用于某些目标，但不能或者更难用于用于其他目标，或者在作用于这些目标的时候会不那么有效。技术人工制品、功能和目标之间的这种概念上的联系，使得我们很难坚持认为技术是价值中立的。即使这一点被认可，技术的价值性也可以用一系列不同的方式来解释。一些作者坚持认为，技术可以具有道德代理。这种说法表明，技术可以自主地、自由地在道德意义上 "行动"，并可以为其行为承担道德责任。

关于技术是否可以具有道德代理的争论始于计算机伦理学(Bechtel 1985; Snapper 1985; Dennett 1997; Floridi & Sanders 2004)，但后来有所扩大。通常情况下，宣称技术（可以）具有道德代理的作者往往重新定义代理（agency）的概念或其与人类意志和自由的联系（例如，Latour 1993；Floridi & Sanders 2004，Verbeek 2011）。这种策略的一个缺点是，它往往模糊了人与技术人工制品之间在道德上的相关区别。更广泛地讲，技术具有道德力量的说法有时似乎成了宣称技术在道德上相关的速记(shorthand)。然而，这忽略了这样一个事实，即技术可以通过其他方式而不是通过具有道德代理来实现价值取向(例如，见Johnson 2006; Radder 2009; Illies & Meijers 2009; Peterson & Spahn 2011)。例如，人们可以宣称技术可以实现（甚至邀约）和制约（甚至抑制）人类的某些行为和某些人类目标，因此在某种程度上是有价值立场的，这不需要宣称技术人工制品有道德代理。关于这场辩论的一个很好的概述可以在Kroes和Verbeek 2014中找到。

关于道德代理和技术的争论现在在智能人工代理的设计方面尤为突出。James Moor（2006）区分了人工代理可能成为或成为道德代理的四种方式。

- 伦理影响代理是对环境产生伦理影响的机器人和计算机系统；这可能是所有人工代理的真实情况。
- 隐性伦理代理是被编程为按照一定的价值观行事的人工代理。
- 显性伦理代理是能够表示伦理类别的机器，并且能够对这些类别进行 "推理"（用机器语言）。
- 完全的伦理代理还拥有一些我们经常认为对人类代理至关重要的特征，比如意识、自由意志和意向性。
从技术上设计完整的伦理代理也许永远不可能，如果成为可能，那么这样做在道德上是否可取可能是个问题（Bostrom & Yudkowsky，2014）。正如Wallach和Allen(2009)所指出的，主要的问题可能不是设计出能够自主运作、能够在与环境的互动中自我调整的人工代理，而是在这种机器中建立足够的、正确的伦理敏感性。

#### 3.3.2 责任
责任一直是技术伦理学的一个核心主题。然而，传统的技术哲学和技术伦理学对责任的讨论往往比较笼统，对工程师是否有可能对其开发的技术承担责任比较悲观。例如，埃卢尔曾把工程师定性为技术的大祭司，他们珍惜技术，但不能引导技术。汉斯-约纳斯(1979[1984])曾认为，技术需要一种以责任为核心的伦理学，因为我们在历史上第一次能够毁灭地球和人类。

在工程伦理学中，工程师的责任往往是结合阐明工程师具体责任的道德守则来讨论的。这种道德准则强调工程师的三种责任。(1)以正直、诚实和称职的方式从事这一职业，(2)对雇主和客户的责任，(3)对公众和社会的责任。关于后者，大多数美国的道德准则认为，工程师 "应该把公众的安全、健康和福利放在首位"。

正如一些作者所指出的那样（Nissenbaum 1996；Johnson & Powers 2005；Swierstra & Jelsma 2006），工程中的个人责任可能很难确定。原因是，哲学文献中讨论过的正确归属个人责任的条件（如行动自由、知识和因果关系）往往不被工程师个人所满足。例如，由于等级或市场的限制，工程师可能会觉得自己不得不以某种方式行事，而负面后果可能很难或无法事先预测。由于一项技术从研发到使用的链条很长，而且这个链条上有很多人参与，因此因果关系条件往往也很难满足。然而，Davis(2012)坚持认为，尽管有这样的困难，工程师个人还是可以并且确实承担了责任。

在这场辩论中，责任的概念是一个关键问题。Davis(2012)，以及Ladd(1991)等人都主张责任的概念不那么注重责备，而是强调承担责任的前瞻性或美德性。但其他许多人则注重责任的后向性概念，强调问责、责难或责任。例如，Zandvoort（2000）曾恳求工程中的责任概念更像法律上的严格责任概念，其中责任的知识条件被严重弱化。Doorn(2012)比较了工程中责任归属的三种观点--基于功绩的观点、基于权利的观点和后果主义的观点，认为后果主义的观点采用前瞻性的责任概念，对工程实践的影响最大。

归属个人责任的困难可能导致 "多手问题"（Problem of Many Hands，PMH）。该词最早由Dennis Thompson（1980年）在一篇关于公职人员责任的文章中提出。该术语用于描述集体环境中个人责任的归属问题。Doorn（2010）提出了一种基于Rawls的反思平衡模型的程序性方法来处理PMH；其他处理PMH的方法包括设计有助于避免PMH的制度或强调组织中的良性行为（van de Poel，Royakers，& Zwart，2015）。

#### 3.3.3 设计
在过去的几十年里，人们不仅越来越关注在使用技术过程中出现的道德问题，而且也越来越关注在设计阶段出现的道德问题。这种发展背后的一个重要考虑是，在设计阶段，技术及其社会后果仍具有可塑性，而在使用阶段，技术或多或少是给定的，消极的社会后果可能更难避免，而积极的效果更难实现。

在计算机伦理学中，为了明确解决设计的伦理性问题，发展了一种被称为价值敏感设计（VSD）的方法。VSD旨在以系统的方式将具有伦理重要性的价值整合到工程设计中（Friedman & Kahn，2003）。这种方法结合了概念、经验和技术调查。还有一系列其他的方法旨在将价值纳入设计中。工程学中的 "为X设计 "方法旨在纳入工具价值（如可维护性、可靠性和成本），但它们也包括可持续设计、包容性设计和情感设计（sustainability, inclusive design, and affective design， Holt & Barnes，2010）。包容性设计的目的是使设计能够被所有的人所接受，例如包括残疾人和老年人（Erlandson，2008）。情感设计的目的是使设计能够唤起使用者的积极情绪，从而促进人类的幸福。Van de Hoven、Vermaas和van de Poel 2015年对各种价值和应用领域的价值敏感设计的现状进行了很好的概述。

如果试图将价值融入设计，可能会遇到价值冲突的问题。最安全的汽车，由于其重量，不可能是最可持续（原文sustainability为typo，应为sustainable）的。在这里，安全和可持续性在汽车设计中发生了冲突。工程师处理这种冲突，在不同的设计要求之间进行权衡的传统方法包括成本效益分析和多标准分析（cost-benefit analysis and multiple criteria analysis）。然而，这样的方法被方法论问题所困扰，就像第2.4节所讨论的那样（Franssen 2005；Hansson 2007）。Van de Poel(2009)讨论了处理设计中价值冲突的各种替代方法，包括设置阈值(满足性)、价值推理、创新和多样性。

#### 3.3.4 技术风险（Technological risks）

技术风险是技术伦理学中传统的伦理问题之一。风险不仅提出了伦理问题，还提出了其他哲学问题，如认识论和决策理论问题（Roeser等人，2012年）。

风险通常被定义为不希望发生的事件的概率和该事件的影响的乘积，尽管也有其他定义（Hansson 2004b）。一般来说，保持技术风险越小越好。风险越大，不良事件的可能性或影响就越大。因此，减少风险是技术发展的一个重要目标，工程道德规范通常将减少风险和设计安全产品的责任赋予工程师。然而，降低风险并不总是可行或可取的。有时是不可行的，因为没有绝对安全的产品和技术。但即使降低风险是可行的，从道德的角度来看，也未必能接受。减少风险往往是有代价的。更安全的产品可能更难使用，更昂贵或更难持续。因此，人们迟早会面临这样一个问题：什么是足够安全的？是什么让风险变得（不可）接受？

处理风险的过程通常分为三个阶段：风险评估、风险评价和风险管理。其中，第二个阶段最明显地与道德相关。然而，风险评估已经涉及到价值判断，例如，首先应该评估哪些风险（Shrader-Frechette 1991）。一个重要的、与道德相关的问题也是确定风险所需的证据程度。在根据一组经验数据确定风险时，人们可能会犯两种错误。人们可以在实际上没有风险的情况下确定风险（第一类错误），也可以在实际上有风险的情况下错误地得出结论说没有风险（第二类错误）。科学传统上以避免第一类错误为目标。有几位作者认为，在风险评估的特定背景下，避免第二类错误往往更为重要（Cranor 1990；Shrader-Frechette 1991）。原因是风险评估的目的不仅仅是建立科学真理，而且还有一个实际的目的，即提供知识，在此基础上可以决定是否应该减少或避免某些技术风险，以保护用户或公众。

风险评价有多种方式进行(例如，见Shrader-Frechette 1985)。一种可能的方法是通过将风险与其他风险或某些标准进行比较来判断风险的可接受性。例如，人们可以将技术风险与自然发生的风险进行比较。然而，这种方法有犯自然主义谬误的危险：自然发生的风险可能（有时）是不可避免的，但这并不一定使它们在道德上可以接受。更一般地说，如果A和B不是决定中的备选方案，那么通过比较技术A的风险和技术B的风险来判断技术A的风险的可接受性往往是可疑的（关于风险推理中的这种和其他谬误，见Hansson 2004a）。

风险评估的第二种方法是风险-成本-效益分析，它的基础是对一项活动的风险和效益进行权衡。如果进行(风险)成本效益分析，可以采用不同的决策标准(Kneese, Ben-David, and Schulze 1983)。根据Hansson (2003: 306)，通常我们采用以下标准:

> 如果而且只有当风险引起的总收益超过总风险，即以结果的概率加权无效性衡量时，风险才是可以接受的。

第三种方法是将风险的接受建立在遭受风险的人被告知这些风险后的同意之上（知情同意）。这种方法的一个问题是，技术风险通常会同时影响大量的人。因此，知情同意可能会导致一个 "僵局社会"（Hansson 2003：300）。

有几位作者根据哲学和伦理学的观点提出了传统风险评估方法的替代方案。Shrader-Frechette(1991)在对现行做法进行哲学批判的基础上，提出了一些风险评估和评价程序的改革建议。Roeser(2012)认为情感在判断风险可接受性中的作用。Hansson提出了以下风险评价的替代原则。

一个人暴露在风险中是可以接受的，如果也只有当这种暴露是一个公平的社会风险承担系统的一部分，并且对她有利的话。（Hansson，2003年：305）

汉森的建议在风险评估中引入了一些传统上没有涉及或仅有少量涉及的道德考虑。这些考虑是个人是否从风险活动中获利，以及考虑风险和利益的分配是否公平。

一些作者批评了技术伦理学中对风险的关注。其中一种批评认为，在一项新技术投入使用之前，我们往往缺乏可靠评估其风险的知识。我们往往不知道某件事情可能出错的概率，有时我们甚至不知道，或至少不完全知道可能出错的原因以及可能产生的负面后果。为了应对这种情况，一些作者提出将社会中新技术的引入设想为一种社会实验，并敦促思考在什么条件下这种实验在道德上是可以接受的（Martin & Schinzinger，2005；van de Poel，2016）。另一种批评认为，对风险的关注导致了技术影响的减少，而这些影响是被考虑的（Swierstra & te Molder，2012）。只有与安全和健康有关的影响（可计算为风险）被考虑，而 "软 "影响，例如社会或心理性质的影响，则被忽视，从而削弱了对新技术的道德评价。